{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Colab GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6AxUAJXwhDi",
        "outputId": "63939267-56e0-4b3c-cd40-d323582f13c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-09-14 18:01:48--  https://drive.usercontent.google.com/download?id=1lhAaeQCmk2y440PmagA0KmIVBIysVMwu&export=download&authuser=0&confirm=t&uuid=3077628e-fc9b-4ef2-8cde-b291040afb30&at=APZUnTU9lSikCSe3NqbxV5MVad5T%3A1708243355040\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# !wget --header=\"Host: drive.usercontent.google.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: en-US,en;q=0.9,ar;q=0.8\" --header=\"Cookie: HSID=Ag2OIHvsd2Wub4C7z; SSID=AWnBcQKwDHiTrZAU1; APISID=pltrFZgE9lJ0o1gq/AN9feEHYvs8oHd519; SAPISID=zgF45F21ZPWzYWZw/AgUMJ8b7QQXuWGn19; __Secure-1PAPISID=zgF45F21ZPWzYWZw/AgUMJ8b7QQXuWGn19; __Secure-3PAPISID=zgF45F21ZPWzYWZw/AgUMJ8b7QQXuWGn19; SID=g.a000fwgYx1PcnW-rFyFhg3x6mQHzCrwXz-KFhoOLogUl7YTWI-uttBbVDRolhF-hY16nwHXw0gACgYKAWISAQASFQHGX2MivNTw_E_toJuIRy6LMpKNOBoVAUF8yKpFSmvq7AMjvEWeNc50Zff40076; __Secure-1PSID=g.a000fwgYx1PcnW-rFyFhg3x6mQHzCrwXz-KFhoOLogUl7YTWI-utbSY2jBY1VXuw8gYl5hIO2QACgYKAXsSAQASFQHGX2MihVCJ1PwLozGqZgdSatM9QhoVAUF8yKpgrsTvI8i_UE-YHpoN7Gx-0076; __Secure-3PSID=g.a000fwgYx1PcnW-rFyFhg3x6mQHzCrwXz-KFhoOLogUl7YTWI-utwVfPl2imdPimZJ9tdDZGQAACgYKAUESAQASFQHGX2MiEJ49mV4jME2kttDAV5hwWBoVAUF8yKp80mIgju1lu-q4nI7VsFDM0076; NID=511=efI9IZpxtyJ7Dw1MAUXU8FlzS5jXGewY4Er8HliWc3A0RSWdgvNDyKY66ETjgRyTGWPbWODSmiSeYSBab5SPHVwqbJxd6ZeGW2f6BkHi61UKksXPH0CVJRM1hKpMjHPU5qw7tboM2Mi87NrosV8COB-GCLulLLbjOoSAEQewTe8NVZ5Owq8IkwvxFGfJkmUKEMkFWrw9yb5nTDl3wbZEsGFI92iEdNTSxSRovNCIPN2US-SCFdQ0m2BtvwdiWZbgnn7dSQ8yPA145Kk2BA-ATpJNJ6SJHEHLQY-9CPail9D5qgJgxR925EUg5RGCpEu9wS5xbA62KTa19wAvbAq7Dk3TWc-iX4p1s7ESFyDC7yMpFxiFPJjqkWwFi_ZfiK2TW2t0TQ60DFBxqOytQaLyHrkEvD-CQPVj6OCOP22cZY0Cu61HaAQgFO9pXH-kJUlywzVdbirJumN5gswyaQ49b3KdLcG0Jb7brOMTM24T2nGtQ10hJzsnTwX7dBk3ujqQrI_DGuURvPassPUrIZ0; AEC=Ae3NU9MOEGeKAZjP6INpOYbyMraWAWztmx5pJB_1ILu1furiTy1K37k15u0; __Secure-1PSIDTS=sidts-CjEBYfD7Z9twEKTWJ9gU7KG-rLbxJGNRQIoG3wH6JVu6yiCC2fsRrm7tN8L6d5WlILrnEAA; __Secure-3PSIDTS=sidts-CjEBYfD7Z9twEKTWJ9gU7KG-rLbxJGNRQIoG3wH6JVu6yiCC2fsRrm7tN8L6d5WlILrnEAA; 1P_JAR=2024-02-18-08; SIDCC=ABTWhQExCxkfmwCkG1RaEgz8U1ZkPeh3HmLMUdMt8S5cNSsLY5U5rAL6wlvq7dtjRw7zrtAbqsFI; __Secure-1PSIDCC=ABTWhQH0jLeRIS6Tu3LS8DXB5Q3gGDq9LTmlk60FKu795Bf0UbzsOcYWVAE96clq5aAL8i724Q0; __Secure-3PSIDCC=ABTWhQHIFcyv3nZYwp78WXEQal71jCE_ZsGT5lXs8VLr7XDIfFqHcLTIPz4HxzJb9ZnYQ5l2s9eU\" --header=\"Connection: keep-alive\" \"https://drive.usercontent.google.com/download?id=1lhAaeQCmk2y440PmagA0KmIVBIysVMwu&export=download&authuser=0&confirm=t&uuid=3077628e-fc9b-4ef2-8cde-b291040afb30&at=APZUnTU9lSikCSe3NqbxV5MVad5T%3A1708243355040\" -c -O 'tennis_court_det_dataset.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9ffXEKfAdC0",
        "outputId": "6e6b071e-e6ec-4cd8-90b7-752b6a39fe0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EYrxojlxoBT",
        "outputId": "436bbb25-1da0-4dc9-f14c-bb9bab96b173"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/tennis_court_det_dataset.zip: Zip archive data, at least v1.0 to extract, compression method=store\n"
          ]
        }
      ],
      "source": [
        "!file /content/drive/MyDrive/tennis_court_det_dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMlnfv4Ouh9d",
        "outputId": "30d6e6bc-7a33-4913-bc55-dc23738458cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unzip successful\n"
          ]
        }
      ],
      "source": [
        "!unzip -q /content/drive/MyDrive/tennis_court_det_dataset.zip\n",
        "print('unzip successful')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7P_SoGz8uh9d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvntvU0Euh9e",
        "outputId": "5c70e5f6-2e02-4ecb-c821-0e97576e79e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RalyMYblpZeg",
        "outputId": "171cae36-8dad-4383-876f-530dfd18d327"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  print(torch.cuda.get_device_name())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsUE_0rmuh9f"
      },
      "source": [
        "# Create Torch Dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfe6sqw8uh9g"
      },
      "source": [
        "Before using the pre-trained models, one must preprocess the image (resize with right resolution/interpolation, apply inference transforms, rescale the values etc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_MnrV8Oxuh9g"
      },
      "outputs": [],
      "source": [
        "class KeypointsDataset(Dataset):\n",
        "     def __init__(self, img_dir, data_file):\n",
        "          self.img_dir = img_dir\n",
        "          with open(data_file, \"r\") as f:\n",
        "               self.data = json.load(f)\n",
        "\n",
        "          # normalize img\n",
        "          self.transforms = transforms.Compose([\n",
        "               transforms.ToPILImage(),\n",
        "               transforms.Resize((224, 224)),\n",
        "               transforms.ToTensor(),\n",
        "               transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "          ])\n",
        "\n",
        "\n",
        "     def __len__(self):\n",
        "          return len(self.data)\n",
        "\n",
        "\n",
        "     def __getitem__(self, idx):\n",
        "          \"\"\"\n",
        "          idx: idx in json file\n",
        "          return: img and keypoints\n",
        "          \"\"\"\n",
        "          item = self.data[idx]\n",
        "          img = cv2.imread(f\"{self.img_dir}/{item['id']}.png\")\n",
        "          h,w = img.shape[:2]\n",
        "          # convert BGR img to RGB img\n",
        "          img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "          img = self.transforms(img)\n",
        "          # convert list to arr\n",
        "          kps = np.array(item['kps']).flatten()\n",
        "          kps = kps.astype(np.float32)\n",
        "\n",
        "          # map kps to og pos after img resize\n",
        "          kps[::2] *= 224.0 / w # adjust x-coordinates\n",
        "          kps[1::2] *= 224.0 / h # adjust y-coordinates\n",
        "\n",
        "          return img, kps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eS6ReSHauh9h"
      },
      "outputs": [],
      "source": [
        "# create dataset from json file\n",
        "train_set = KeypointsDataset(\"data/images\", \"data/data_train.json\")\n",
        "val_set = KeypointsDataset(\"data/images\", \"data/data_val.json\")\n",
        "\n",
        "# load dataset\n",
        "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rYGX-ENuh9i"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Yaik-V1Yuh9i"
      },
      "outputs": [],
      "source": [
        "model = models.resnet50(pretrained=True)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 14*2) # 14 (x,y) kps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FgVyVwuzuh9i"
      },
      "outputs": [],
      "source": [
        "# move to CUDA, if available\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT9TcVe2uh9j"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kN3lirYVuh9j"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iu3LLG8Azjf",
        "outputId": "fab153e5-6090-4f06-ea9d-f79440dff890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Img 1/829, Loss: 14837.95\n",
            "Epoch 1, Img 251/829, Loss: 6497.51\n",
            "Epoch 1, Img 501/829, Loss: 2177.45\n",
            "Epoch 1, Img 751/829, Loss: 404.85\n",
            "Epoch 1, Img 829/829, Loss: 210.32\n",
            "Epoch 2, Img 1/829, Loss: 226.15\n",
            "Epoch 2, Img 251/829, Loss: 117.38\n",
            "Epoch 2, Img 501/829, Loss: 37.18\n",
            "Epoch 2, Img 751/829, Loss: 88.43\n",
            "Epoch 2, Img 829/829, Loss: 33.63\n",
            "Epoch 3, Img 1/829, Loss: 38.90\n",
            "Epoch 3, Img 251/829, Loss: 34.47\n",
            "Epoch 3, Img 501/829, Loss: 28.75\n",
            "Epoch 3, Img 751/829, Loss: 25.72\n",
            "Epoch 3, Img 829/829, Loss: 15.00\n",
            "Epoch 4, Img 1/829, Loss: 15.56\n",
            "Epoch 4, Img 251/829, Loss: 28.08\n",
            "Epoch 4, Img 501/829, Loss: 36.58\n",
            "Epoch 4, Img 751/829, Loss: 13.93\n",
            "Epoch 4, Img 829/829, Loss: 6.79\n",
            "Epoch 5, Img 1/829, Loss: 26.81\n",
            "Epoch 5, Img 251/829, Loss: 6.04\n",
            "Epoch 5, Img 501/829, Loss: 14.96\n",
            "Epoch 5, Img 751/829, Loss: 11.87\n",
            "Epoch 5, Img 829/829, Loss: 15.04\n",
            "Epoch 6, Img 1/829, Loss: 162.78\n",
            "Epoch 6, Img 251/829, Loss: 4.40\n",
            "Epoch 6, Img 501/829, Loss: 5.44\n",
            "Epoch 6, Img 751/829, Loss: 3.88\n",
            "Epoch 6, Img 829/829, Loss: 5.56\n",
            "Epoch 7, Img 1/829, Loss: 10.41\n",
            "Epoch 7, Img 251/829, Loss: 2.47\n",
            "Epoch 7, Img 501/829, Loss: 7.11\n",
            "Epoch 7, Img 751/829, Loss: 1.47\n",
            "Epoch 7, Img 829/829, Loss: 7.29\n",
            "Epoch 8, Img 1/829, Loss: 5.72\n",
            "Epoch 8, Img 251/829, Loss: 1.91\n",
            "Epoch 8, Img 501/829, Loss: 62.17\n",
            "Epoch 8, Img 751/829, Loss: 0.80\n",
            "Epoch 8, Img 829/829, Loss: 9.16\n",
            "Epoch 9, Img 1/829, Loss: 3.02\n",
            "Epoch 9, Img 251/829, Loss: 7.99\n",
            "Epoch 9, Img 501/829, Loss: 3.77\n",
            "Epoch 9, Img 751/829, Loss: 2.52\n",
            "Epoch 9, Img 829/829, Loss: 9.83\n",
            "Epoch 10, Img 1/829, Loss: 1.14\n",
            "Epoch 10, Img 251/829, Loss: 1.54\n",
            "Epoch 10, Img 501/829, Loss: 1.53\n",
            "Epoch 10, Img 751/829, Loss: 2.11\n",
            "Epoch 10, Img 829/829, Loss: 4.75\n",
            "Epoch 11, Img 1/829, Loss: 70.12\n",
            "Epoch 11, Img 251/829, Loss: 0.85\n",
            "Epoch 11, Img 501/829, Loss: 2.19\n",
            "Epoch 11, Img 751/829, Loss: 1.70\n",
            "Epoch 11, Img 829/829, Loss: 5.28\n",
            "Epoch 12, Img 1/829, Loss: 0.60\n",
            "Epoch 12, Img 251/829, Loss: 2.42\n",
            "Epoch 12, Img 501/829, Loss: 1.84\n",
            "Epoch 12, Img 751/829, Loss: 2.24\n",
            "Epoch 12, Img 829/829, Loss: 9.29\n",
            "Epoch 13, Img 1/829, Loss: 2.47\n",
            "Epoch 13, Img 251/829, Loss: 1.39\n",
            "Epoch 13, Img 501/829, Loss: 2.14\n",
            "Epoch 13, Img 751/829, Loss: 1.12\n",
            "Epoch 13, Img 829/829, Loss: 2.69\n",
            "Epoch 14, Img 1/829, Loss: 1.48\n",
            "Epoch 14, Img 251/829, Loss: 1.32\n",
            "Epoch 14, Img 501/829, Loss: 2.15\n",
            "Epoch 14, Img 751/829, Loss: 2.03\n",
            "Epoch 14, Img 829/829, Loss: 5.93\n",
            "Epoch 15, Img 1/829, Loss: 2.65\n",
            "Epoch 15, Img 251/829, Loss: 1.03\n",
            "Epoch 15, Img 501/829, Loss: 2.25\n",
            "Epoch 15, Img 751/829, Loss: 0.62\n",
            "Epoch 15, Img 829/829, Loss: 1.56\n",
            "Epoch 16, Img 1/829, Loss: 0.82\n",
            "Epoch 16, Img 251/829, Loss: 22.20\n",
            "Epoch 16, Img 501/829, Loss: 1.32\n",
            "Epoch 16, Img 751/829, Loss: 1.86\n",
            "Epoch 16, Img 829/829, Loss: 1.37\n",
            "Epoch 17, Img 1/829, Loss: 3.00\n",
            "Epoch 17, Img 251/829, Loss: 4.74\n",
            "Epoch 17, Img 501/829, Loss: 2.67\n",
            "Epoch 17, Img 751/829, Loss: 1.44\n",
            "Epoch 17, Img 829/829, Loss: 1.64\n",
            "Epoch 18, Img 1/829, Loss: 0.67\n",
            "Epoch 18, Img 251/829, Loss: 1.23\n",
            "Epoch 18, Img 501/829, Loss: 0.71\n",
            "Epoch 18, Img 751/829, Loss: 2.80\n",
            "Epoch 18, Img 829/829, Loss: 0.72\n",
            "Epoch 19, Img 1/829, Loss: 0.88\n",
            "Epoch 19, Img 251/829, Loss: 2.34\n",
            "Epoch 19, Img 501/829, Loss: 0.60\n",
            "Epoch 19, Img 751/829, Loss: 0.53\n",
            "Epoch 19, Img 829/829, Loss: 0.64\n",
            "Epoch 20, Img 1/829, Loss: 2.28\n",
            "Epoch 20, Img 251/829, Loss: 9.26\n",
            "Epoch 20, Img 501/829, Loss: 1.06\n",
            "Epoch 20, Img 751/829, Loss: 1.70\n",
            "Epoch 20, Img 829/829, Loss: 0.59\n"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    for i, (imgs,kps) in enumerate(train_loader):\n",
        "        imgs = imgs.to(device)\n",
        "        kps = kps.to(device)\n",
        "\n",
        "        # flush out any pre-computed gradients\n",
        "        optimizer.zero_grad()\n",
        "        # use model to predict kps on imgs\n",
        "        preds = model(imgs)\n",
        "        loss = criterion(kps, preds)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i % 250 == 0) or ((i+1) % len(train_loader) == 0):\n",
        "            print(f\"Epoch {epoch + 1}, Img {i+1}/{len(train_loader)}, Loss: {loss.item():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fM630N42qhha"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/output/keypoints_model.pth\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
